# SENSE-Edge

**Sensor-to-Image Neuro-Symbolic & Visionâ€“Language Framework for Explainable Environmental Intelligence on Edge Devices**

---

## ğŸ“Œ Overview

SENSE-Edge is an **agentic, neuro-symbolic, and Visionâ€“Languageâ€“driven framework** designed for real-time, explainable environmental monitoring on **edge devices**.

It introduces a novel **Sensor-to-Image Encoding** pipeline that transforms heterogeneous environmental sensor data (air, water, climate, indoor) into structured **visual representations** compatible with large vision models.

Unlike traditional time-series approaches, SENSE-Edge integrates:

- Sensor-to-Image data transformation  
- Visionâ€“Language Model (VLM) perception  
- Neuro-symbolic reasoning & ontologies  
- Agentic autonomous decision-making  
- Edge-focused, low-power deployment  

The framework supports **explainable, trustworthy, and scalable environmental intelligence**, aligned with sustainable development and AI transparency principles.

---

## ğŸ¯ Research Objectives

SENSE-Edge aims to:

- Convert heterogeneous sensor streams into meaningful **2D visual representations**  
  *(GAF, RP, Spectrograms, Heatmaps)*  
- Leverage **Visionâ€“Language Models** (e.g., CLIP / ViT) for perception of synthetic sensor imagery  
- Integrate a **neuro-symbolic reasoning layer** using ontology + ASP logic  
- Enable **agentic, autonomous** monitoring and alert generation  
- Operate efficiently on **edge devices** (e.g., NVIDIA Jetson)  
- Provide **explainable decisions** for public and environmental stakeholders  

---

## ğŸ— System Architecture

Sensors (Air, Water, Climate, Indoor)
â†“
Sensor-to-Image Encoding
(GAF, RP, Spectrograms, Heatmaps)
â†“
Visionâ€“Language Model
(CLIP / ViT)
â†“
Embeddings â†’ Symbolic Conversion
â†“
Neuro-Symbolic Reasoning (ASP)
â†“
Agentic Decision & Logging
â†“
Explainable Output

yaml
Copy code

### Three Main Logic Layers

- **Ontology Layer** â€“ Defines environmental concepts, ranges, thresholds & relationships  
- **Reasoning Layer** â€“ ASP-based probabilistic neuro-symbolic inference  
- **Agentic Layer** â€“ Autonomous triggering, monitoring, action, explanation  

---

## ğŸ§ª Proof-of-Concept

Initial experiments were conducted using the **Beijing PM2.5 Dataset**:

- Sensor windows converted to visual form  
- Processed using an adapted **CLIP** model  
- Achieved **41% accuracy** on a **5-class** problem  
- Approaching published state-of-the-art (**~48.8%**)  

This demonstrates early feasibility of the vision-based method.

---

## ğŸ§° Technologies & Methods

| Component     | Tool / Method                         |
|----------|----------------------------------------|
| Encoding  | GAF, RP, Spectrograms, Heatmaps |
| VLM       | CLIP / Vision Transformer        |
| Reasoning | Answer Set Programming (ASP)    |
| Ontology  | Domain-specific environmental schema |
| Deployment| Jetson / Edge AI / Low-power systems |
| Evaluation| Accuracy, latency, memory, energy |

---

## ğŸ“ Repository Structure *(Work in Progress)*

```bash
SENSE-Edge/
â”‚
â”œâ”€â”€ data/                # Sample & processed datasets
â”œâ”€â”€ encoding/            # Sensor-to-Image methods
â”œâ”€â”€ vlm/                 # CLIP / ViT processing
â”œâ”€â”€ reasoning/           # Neuro-symbolic module (ASP)
â”œâ”€â”€ agent/               # Agentic control layer
â”œâ”€â”€ experiments/         # Evaluation & results
â”œâ”€â”€ docs/                # Diagrams & concept notes
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ”„ Current Status
âœ… Conceptual architecture defined

âœ… Encoding pipeline implemented

âœ… VLM prototype tested (CLIP)

âœ… Initial evaluation completed

ğŸ”„ Neuro-symbolic module expansion

ğŸ”„ Edge deployment optimisation

ğŸ”„ Multi-domain testing

This repository will be continuously updated with experiments, improvements, and documentation.

ğŸ” References
Zhong et al., Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting, ICML 2025

More references will be added in the /docs folder

ğŸ‘¤ Author
